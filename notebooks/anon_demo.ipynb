{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd92e48f-9b7f-4979-8c66-0b67b0ffc44e",
   "metadata": {},
   "source": [
    "### Anon demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d2c905-cf28-4e56-aaeb-bcffe6423a7f",
   "metadata": {},
   "source": [
    "Assignment:\n",
    "\n",
    "1) Anon the user field and create a mapping\n",
    "2) Anon the IP address field (which is expressed as a string, but can also map to a 32bit integer. [yet sparsely so in our data]\n",
    "3) Make a vectors of the anonymous data and clear data\n",
    "4) Generate a \"ragged array\" of the data in this form (i.e. dump out the records which are of variable length)\n",
    "Bonus) Tell us which users jobs fail and how often as an absolute number, and as a fraction of the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f630042-aabb-45da-ad77-3a0b74b1f36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Anonymized preview ===\n",
      "    user            ip  failed  job_id               user_anon         ip_anon\n",
      "0  alice      10.1.2.3    True       1  fqeh3jdtz2iyvzdgq3ju4q  250.82.115.216\n",
      "1    bob      10.1.2.4   False       2  nz6qbszgpehbffbuae62te  93.175.157.185\n",
      "2  alice    3232235777    True       3  fqeh3jdtz2iyvzdgq3ju4q   20.94.214.208\n",
      "3  carol  192.168.1.10   False       4  px7kwwhxx52uon5bl6ijtu  77.173.143.144\n",
      "4   None     not-an-ip    True       5                    None            None\n",
      "\n",
      "=== User mapping (first 3) ===\n",
      "{'alice': 'fqeh3jdtz2iyvzdgq3ju4q', 'bob': 'nz6qbszgpehbffbuae62te', 'carol': 'px7kwwhxx52uon5bl6ijtu'}\n",
      "\n",
      "=== IP mapping (first 3) ===\n",
      "{'10.1.2.3': {'anon_ip': '250.82.115.216', 'anon_ip_int': 4199707608}, '10.1.2.4': {'anon_ip': '93.175.157.185', 'anon_ip_int': 1571790265}, 3232235777: {'anon_ip': '20.94.214.208', 'anon_ip_int': 341759696}}\n",
      "\n",
      "=== X_anon shape / X_clear shape === (5, 2) (5, 3)\n",
      "\n",
      "=== Failure stats ===\n",
      "    user  fails  total  frac_of_user_rows  frac_of_all_rows\n",
      "0  alice      2      2                1.0               0.4\n",
      "3    NaN      1      1                1.0               0.2\n",
      "1    bob      0      1                0.0               0.0\n",
      "2  carol      0      1                0.0               0.0\n",
      "\n",
      "Ragged JSONL written to: ../data/anon_ragged.jsonl\n"
     ]
    }
   ],
   "source": [
    "# --- Imports\n",
    "import os, hmac, hashlib, base64, ipaddress, json\n",
    "from typing import Any, Dict, Iterable, List, Tuple, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "DATA_DIR = \"../data\"\n",
    "\n",
    "# ==========\n",
    "# 0) Key setup (replace with KMS in prod)\n",
    "# ==========\n",
    "KEY_HEX = hashlib.sha256(b\"random-demo-key\").hexdigest()\n",
    "KEY = bytes.fromhex(KEY_HEX)\n",
    "\n",
    "# ==========\n",
    "# 1) HMAC tokenization (deterministic pseudonym)\n",
    "# ==========\n",
    "def _to_bytes(x: Any) -> bytes:\n",
    "    if x is None:\n",
    "        return b\"\"\n",
    "    try:\n",
    "        if pd.isna(x):\n",
    "            return b\"\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    if isinstance(x, bytes):\n",
    "        return x\n",
    "    return str(x).encode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "def hmac_token(value: Any, key: bytes, out_len: int = 22) -> str:\n",
    "    mac = hmac.new(key, _to_bytes(value), hashlib.sha256).digest()\n",
    "    return base64.b32encode(mac).decode(\"ascii\").rstrip(\"=\")[:out_len].lower()\n",
    "\n",
    "# ==========\n",
    "# 2) IPv4 anonymization (deterministic 32-bit remap → dotted-quad)\n",
    "#    Note: This preserves IPv4 format, not subnet/prefix structure (MVP-friendly).\n",
    "# ==========\n",
    "def ip_str_to_int(ip_str: str) -> Optional[int]:\n",
    "    if ip_str is None or (isinstance(ip_str, float) and np.isnan(ip_str)):\n",
    "        return None\n",
    "    try:\n",
    "        return int(ipaddress.IPv4Address(str(ip_str)))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def int_to_ip_str(ip_int: int) -> str:\n",
    "    return str(ipaddress.IPv4Address(ip_int & 0xFFFFFFFF))\n",
    "\n",
    "def anon_ipv4(ip_value: Any, key: bytes) -> Optional[str]:\n",
    "    \"\"\"Map any IPv4 string (or int-like) to a deterministic anon IPv4 string.\"\"\"\n",
    "    if ip_value is None or (isinstance(ip_value, float) and np.isnan(ip_value)):\n",
    "        return None\n",
    "    # Normalize to 32-bit int\n",
    "    ip_int = None\n",
    "    if isinstance(ip_value, (int, np.integer)):\n",
    "        ip_int = int(ip_value)\n",
    "    else:\n",
    "        ip_int = ip_str_to_int(str(ip_value))\n",
    "    if ip_int is None:\n",
    "        return None\n",
    "    mac = hmac.new(key, ip_int.to_bytes(4, \"big\", signed=False), hashlib.sha256).digest()\n",
    "    anon_int = int.from_bytes(mac[:4], \"big\")  # 32-bit remap\n",
    "    return int_to_ip_str(anon_int)\n",
    "\n",
    "def anon_ipv4_both(ip_value: Any, key: bytes) -> Tuple[Optional[str], Optional[int]]:\n",
    "    \"\"\"Return (anon_ip_str, anon_ip_int) for convenience.\"\"\"\n",
    "    s = anon_ipv4(ip_value, key)\n",
    "    return s, (int(ipaddress.IPv4Address(s)) if s else None)\n",
    "\n",
    "# ==========\n",
    "# 3) Assignment functions\n",
    "# ==========\n",
    "\n",
    "# 1) Anon the user field and create a mapping\n",
    "def anonymize_user_with_mapping(\n",
    "    df: pd.DataFrame, user_col: str, key: bytes = KEY, token_len: int = 22\n",
    ") -> Tuple[pd.Series, Dict[Any, str]]:\n",
    "    mapping: Dict[Any, str] = {}\n",
    "    def _map(v):\n",
    "        if v in mapping:\n",
    "            return mapping[v]\n",
    "        t = hmac_token(v, key, out_len=token_len) if (v is not None and not (isinstance(v, float) and np.isnan(v))) else v\n",
    "        mapping[v] = t\n",
    "        return t\n",
    "    anon_series = df[user_col].map(_map)\n",
    "    return anon_series, mapping\n",
    "\n",
    "# 2) Anon the IP address field (works if your data is strings or 32-bit ints)\n",
    "def anonymize_ip_with_mapping(\n",
    "    df: pd.DataFrame, ip_col: str, key: bytes = KEY\n",
    ") -> Tuple[pd.Series, Dict[Any, Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      anon_ip_str_series,\n",
    "      mapping: { original -> {\"anon_ip\": str, \"anon_ip_int\": int} }\n",
    "    \"\"\"\n",
    "    mapping: Dict[Any, Dict[str, Any]] = {}\n",
    "    def _map(v):\n",
    "        if v in mapping:\n",
    "            return mapping[v][\"anon_ip\"]\n",
    "        s, i = anon_ipv4_both(v, key)\n",
    "        mapping[v] = {\"anon_ip\": s, \"anon_ip_int\": i}\n",
    "        return s\n",
    "    anon_series = df[ip_col].map(_map)\n",
    "    return anon_series, mapping\n",
    "\n",
    "# 3) Make vectors of the anonymous data and clear data\n",
    "def make_vectors(\n",
    "    df: pd.DataFrame,\n",
    "    anon_cols: Iterable[str],\n",
    "    clear_cols: Iterable[str],\n",
    "    user_col: Optional[str] = None,\n",
    "    ip_col: Optional[str] = None,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Build simple numeric vectors:\n",
    "      - Strings are hashed to integers (stable) with SHA256 and truncated.\n",
    "      - Numbers remain as-is; datetime → epoch ms; bool → 0/1; None/NaN → -1.\n",
    "    Produces (X_anon, X_clear) aligned by row.\n",
    "    \"\"\"\n",
    "    def _numify(val: Any) -> float:\n",
    "        if val is None or (isinstance(val, float) and np.isnan(val)):\n",
    "            return -1.0\n",
    "        if isinstance(val, (int, float, np.integer, np.floating)):\n",
    "            return float(val)\n",
    "        if isinstance(val, (pd.Timestamp, np.datetime64)):\n",
    "            try:\n",
    "                return pd.to_datetime(val).value / 1e6  # ms\n",
    "            except Exception:\n",
    "                pass\n",
    "        if isinstance(val, (bool, np.bool_)):\n",
    "            return 1.0 if val else 0.0\n",
    "        # strings/other → deterministic hash to 32-bit range\n",
    "        h = hashlib.sha256(_to_bytes(val)).digest()\n",
    "        return float(int.from_bytes(h[:4], \"big\"))  # 0..2^32-1\n",
    "\n",
    "    X_anon = np.array([[ _numify(df.loc[i, c]) for c in anon_cols ] for i in df.index], dtype=float)\n",
    "    X_clear = np.array([[ _numify(df.loc[i, c]) for c in clear_cols ] for i in df.index], dtype=float)\n",
    "    return X_anon, X_clear\n",
    "\n",
    "# 4) Generate a ragged array (variable-length dump)\n",
    "def to_ragged_records(df: pd.DataFrame, include_cols: Iterable[str]) -> List[List[Tuple[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Represent each record as a list of (key, value) for only present (non-null) columns.\n",
    "    This yields variable-length records → a ragged array.\n",
    "    \"\"\"\n",
    "    ragged: List[List[Tuple[str, Any]]] = []\n",
    "    for _, row in df.iterrows():\n",
    "        items = []\n",
    "        for c in include_cols:\n",
    "            v = row.get(c, None)\n",
    "            if v is not None and not (isinstance(v, float) and np.isnan(v)):\n",
    "                items.append((c, v))\n",
    "        ragged.append(items)\n",
    "    return ragged\n",
    "\n",
    "def dump_ragged_jsonl(ragged: List[List[Tuple[str, Any]]], path: str) -> None:\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for rec in ragged:\n",
    "            # dump as list of {\"k\":..., \"v\":...} for clarity\n",
    "            f.write(json.dumps([{\"k\": k, \"v\": v} for k, v in rec], ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# Bonus) Failure stats per user (absolute and fraction of dataset)\n",
    "def failure_stats(\n",
    "    df: pd.DataFrame,\n",
    "    user_col: str,\n",
    "    failed_col: str,          # boolean or {0/1} or {\"success\"/\"failed\"} etc.\n",
    "    normalize: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with:\n",
    "      user, fails, total, frac_of_user_rows, frac_of_all_rows\n",
    "    \"\"\"\n",
    "    # normalize to boolean\n",
    "    failed = df[failed_col]\n",
    "    if failed.dtype == \"O\":\n",
    "        failed_bool = failed.astype(str).str.lower().isin([\"1\",\"true\",\"t\",\"yes\",\"y\",\"failed\",\"fail\"])\n",
    "    else:\n",
    "        failed_bool = failed.astype(bool)\n",
    "\n",
    "    g = df.assign(__failed=failed_bool).groupby(user_col, dropna=False)\n",
    "    per_user = g[\"__failed\"].agg(fails=\"sum\", total=\"count\")\n",
    "    per_user[\"frac_of_user_rows\"] = per_user[\"fails\"] / per_user[\"total\"]\n",
    "    per_user = per_user.reset_index()\n",
    "    if normalize:\n",
    "        all_total = len(df)\n",
    "        per_user[\"frac_of_all_rows\"] = per_user[\"fails\"] / max(all_total, 1)\n",
    "    else:\n",
    "        per_user[\"frac_of_all_rows\"] = np.nan\n",
    "    return per_user.sort_values([\"fails\",\"total\"], ascending=[False, False])\n",
    "\n",
    "# ==========\n",
    "# Demo wiring (remove/adjust for your data)\n",
    "# ==========\n",
    "# Example schema guess; rename these to match your columns:\n",
    "USER_COL = \"user\"\n",
    "IP_COL   = \"ip\"        # can be IPv4 string or 32-bit int in your data\n",
    "FAILED_COL = \"failed\"  # boolean-ish; see failure_stats()\n",
    "\n",
    "# Minimal demo DataFrame (replace with your read_parquet)\n",
    "demo = pd.DataFrame({\n",
    "    USER_COL:   [\"alice\",\"bob\",\"alice\",\"carol\", None],\n",
    "    IP_COL:     [\"10.1.2.3\", \"10.1.2.4\", 3232235777, \"192.168.1.10\", \"not-an-ip\"],  # third is 192.168.1.1 as int\n",
    "    FAILED_COL: [True, False, True, False, True],\n",
    "    \"job_id\":   [1,2,3,4,5],\n",
    "})\n",
    "\n",
    "# 1) User anon + mapping\n",
    "demo[\"user_anon\"], user_map = anonymize_user_with_mapping(demo, USER_COL, key=KEY)\n",
    "# 2) IP anon + mapping\n",
    "demo[\"ip_anon\"], ip_map = anonymize_ip_with_mapping(demo, IP_COL, key=KEY)\n",
    "\n",
    "# 3) Vectors (choose any columns you want included)\n",
    "anon_cols  = [\"user_anon\",\"ip_anon\"]\n",
    "clear_cols = [USER_COL, IP_COL, FAILED_COL]\n",
    "X_anon, X_clear = make_vectors(demo, anon_cols=anon_cols, clear_cols=clear_cols)\n",
    "\n",
    "# 4) Ragged array dump (variable-length records)\n",
    "ragged = to_ragged_records(demo, include_cols=[USER_COL,\"user_anon\",IP_COL,\"ip_anon\",\"job_id\",FAILED_COL])\n",
    "dump_ragged_jsonl(ragged, f\"{DATA_DIR}/anon_ragged.jsonl\")\n",
    "\n",
    "# Bonus) Failure stats per user\n",
    "fails = failure_stats(demo, user_col=USER_COL, failed_col=FAILED_COL, normalize=True)\n",
    "\n",
    "print(\"=== Anonymized preview ===\")\n",
    "print(demo)\n",
    "print(\"\\n=== User mapping (first 3) ===\")\n",
    "print(dict(list(user_map.items())[:3]))\n",
    "print(\"\\n=== IP mapping (first 3) ===\")\n",
    "print(dict(list(ip_map.items())[:3]))\n",
    "print(\"\\n=== X_anon shape / X_clear shape ===\", X_anon.shape, X_clear.shape)\n",
    "print(\"\\n=== Failure stats ===\")\n",
    "print(fails)\n",
    "\n",
    "print(f\"\\nRagged JSONL written to: {DATA_DIR}/anon_ragged.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e6be76a-f8a8-4bf8-9edf-2eae13046ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 files\n",
      "Read 2 and loaded demo:74500\n",
      "1) Anonymized user with mapping\n",
      "2) Anonymized IP with mapping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1957/841739167.py:218: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo[\"user_anon\"], user_map = anonymize_user_with_mapping(demo, USER_COL, key=KEY)\n",
      "/tmp/ipykernel_1957/841739167.py:221: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo[\"ip_anon\"], ip_map = anonymize_ip_with_mapping(demo, IP_COL, key=KEY)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2) Made vectors\n",
      "4) Made ragged array\n",
      "=== Anonymized preview ===\n",
      "      @timestamp @version              AccountingGroup AccountingGroupOSG  \\\n",
      "0            NaT        1  group_uboone.prod.uboonepro               None   \n",
      "1            NaT        1  group_uboone.prod.uboonepro               None   \n",
      "2            NaT        1  group_uboone.prod.uboonepro               None   \n",
      "3            NaT        1  group_uboone.prod.uboonepro               None   \n",
      "4            NaT        1   group_icarus.pro.icaruspro               None   \n",
      "...          ...      ...                          ...                ...   \n",
      "74495        NaT        1   group_icarus.pro.icaruspro               None   \n",
      "74496        NaT        1   group_icarus.pro.icaruspro               None   \n",
      "74497        NaT        1  group_uboone.prod.uboonepro               None   \n",
      "74498        NaT        1   group_icarus.pro.icaruspro               None   \n",
      "74499        NaT        1   group_icarus.pro.icaruspro               None   \n",
      "\n",
      "      AcctGroup AcctGroupUser AllowOpportunistic  \\\n",
      "0          None          None               None   \n",
      "1          None          None               None   \n",
      "2          None          None               None   \n",
      "3          None          None               None   \n",
      "4          None          None               None   \n",
      "...         ...           ...                ...   \n",
      "74495      None          None               None   \n",
      "74496      None          None               None   \n",
      "74497      None          None               None   \n",
      "74498      None          None               None   \n",
      "74499      None          None               None   \n",
      "\n",
      "                                                    Args Arguments  \\\n",
      "0      --nfile 1 --group uboone -g -c wrapper.fcl --u...      None   \n",
      "1      --nfile 1 --group uboone -g -c wrapper.fcl --u...      None   \n",
      "2      --nfile 1 --group uboone -g -c wrapper.fcl --u...      None   \n",
      "3      --nfile 1 --group uboone -g -c wrapper.fcl --u...      None   \n",
      "4      --debug --find_setups --source-unquote /cvmfs/...      None   \n",
      "...                                                  ...       ...   \n",
      "74495  --debug --find_setups --source-unquote /cvmfs/...      None   \n",
      "74496  --debug --find_setups --source-unquote /cvmfs/...      None   \n",
      "74497  --nfile 1 --group uboone -g -c wrapper.fcl --u...      None   \n",
      "74498  --debug --find_setups --source-unquote /cvmfs/...      None   \n",
      "74499  --debug --find_setups --source-unquote /cvmfs/...      None   \n",
      "\n",
      "      AutoClusterAttrs  ...  x509UserProxyExpiration  \\\n",
      "0                 None  ...      2024-01-08 00:58:05   \n",
      "1                 None  ...      2024-01-08 00:58:05   \n",
      "2                 None  ...      2024-01-08 00:58:05   \n",
      "3                 None  ...      2024-01-08 00:58:05   \n",
      "4                 None  ...      2024-01-07 20:58:02   \n",
      "...                ...  ...                      ...   \n",
      "74495             None  ...      2024-01-08 04:58:09   \n",
      "74496             None  ...      2024-01-08 00:58:02   \n",
      "74497             None  ...      2024-01-08 04:58:05   \n",
      "74498             None  ...      2024-01-08 04:58:09   \n",
      "74499             None  ...      2024-01-08 04:58:09   \n",
      "\n",
      "      x509UserProxyExpiration_ms  x509UserProxyFQAN  x509UserProxyFirstFQAN  \\\n",
      "0                   1.704697e+12               None                    None   \n",
      "1                   1.704697e+12               None                    None   \n",
      "2                   1.704697e+12               None                    None   \n",
      "3                   1.704697e+12               None                    None   \n",
      "4                   1.704683e+12               None                    None   \n",
      "...                          ...                ...                     ...   \n",
      "74495               1.704711e+12               None                    None   \n",
      "74496               1.704697e+12               None                    None   \n",
      "74497               1.704711e+12               None                    None   \n",
      "74498               1.704711e+12               None                    None   \n",
      "74499               1.704711e+12               None                    None   \n",
      "\n",
      "       x509UserProxyVOName               x509userproxy  \\\n",
      "0                     None  uboonepro.Production.proxy   \n",
      "1                     None  uboonepro.Production.proxy   \n",
      "2                     None  uboonepro.Production.proxy   \n",
      "3                     None  uboonepro.Production.proxy   \n",
      "4                     None  icaruspro.Production.proxy   \n",
      "...                    ...                         ...   \n",
      "74495                 None  icaruspro.Production.proxy   \n",
      "74496                 None  icaruspro.Production.proxy   \n",
      "74497                 None  uboonepro.Production.proxy   \n",
      "74498                 None  icaruspro.Production.proxy   \n",
      "74499                 None  icaruspro.Production.proxy   \n",
      "\n",
      "                                    x509userproxysubject  xcount  \\\n",
      "0      /DC=org/DC=incommon/C=US/ST=Illinois/O=Fermi R...    None   \n",
      "1      /DC=org/DC=incommon/C=US/ST=Illinois/O=Fermi R...    None   \n",
      "2      /DC=org/DC=incommon/C=US/ST=Illinois/O=Fermi R...    None   \n",
      "3      /DC=org/DC=incommon/C=US/ST=Illinois/O=Fermi R...    None   \n",
      "4      /DC=org/DC=incommon/C=US/ST=Illinois/O=Fermi R...    None   \n",
      "...                                                  ...     ...   \n",
      "74495  /DC=org/DC=incommon/C=US/ST=Illinois/O=Fermi R...    None   \n",
      "74496  /DC=org/DC=incommon/C=US/ST=Illinois/O=Fermi R...    None   \n",
      "74497  /DC=org/DC=incommon/C=US/ST=Illinois/O=Fermi R...    None   \n",
      "74498  /DC=org/DC=incommon/C=US/ST=Illinois/O=Fermi R...    None   \n",
      "74499  /DC=org/DC=incommon/C=US/ST=Illinois/O=Fermi R...    None   \n",
      "\n",
      "                    user_anon         ip_anon  \n",
      "0      jjomvaqmdpnw5ptv2fwmfx  62.137.252.165  \n",
      "1      jjomvaqmdpnw5ptv2fwmfx  62.137.252.165  \n",
      "2      jjomvaqmdpnw5ptv2fwmfx  62.137.252.165  \n",
      "3      jjomvaqmdpnw5ptv2fwmfx  62.137.252.165  \n",
      "4      6pg5b5knt2ekgh2aqcpzno   67.222.20.222  \n",
      "...                       ...             ...  \n",
      "74495  6pg5b5knt2ekgh2aqcpzno   67.222.20.222  \n",
      "74496  6pg5b5knt2ekgh2aqcpzno   67.222.20.222  \n",
      "74497  jjomvaqmdpnw5ptv2fwmfx  62.137.252.165  \n",
      "74498  6pg5b5knt2ekgh2aqcpzno   67.222.20.222  \n",
      "74499  6pg5b5knt2ekgh2aqcpzno   67.222.20.222  \n",
      "\n",
      "[74500 rows x 597 columns]\n",
      "\n",
      "=== User mapping (first 3) ===\n",
      "{'uboonepro@fnal.gov': 'jjomvaqmdpnw5ptv2fwmfx', 'icaruspro@fnal.gov': '6pg5b5knt2ekgh2aqcpzno', 'gputnam@fnal.gov': '5bnjxybzd3jzlwpj3l72o3'}\n",
      "\n",
      "=== IP mapping (first 3) ===\n",
      "{'131.225.240.146': {'anon_ip': '62.137.252.165', 'anon_ip_int': 1049230501}, '131.225.240.90': {'anon_ip': '67.222.20.222', 'anon_ip_int': 1138627806}, '131.225.240.140': {'anon_ip': '154.178.162.238', 'anon_ip_int': 2595398382}}\n",
      "\n",
      "=== X_anon shape / X_clear shape === (74500, 2) (74500, 3)\n",
      "\n",
      "=== Failure stats ===\n",
      "                   User  fails  total  frac_of_user_rows  frac_of_all_rows\n",
      "24   uboonepro@fnal.gov  28874  28874                1.0          0.387570\n",
      "9    icaruspro@fnal.gov  16769  16769                1.0          0.225087\n",
      "1       cmsgli@fnal.gov   8603   8603                1.0          0.115477\n",
      "17     novapro@fnal.gov   6344   6344                1.0          0.085154\n",
      "6      gputnam@fnal.gov   5016   5016                1.0          0.067329\n",
      "5       gm2pro@fnal.gov   2344   2344                1.0          0.031463\n",
      "11     laliaga@fnal.gov   2279   2279                1.0          0.030591\n",
      "10      imawby@fnal.gov   2042   2042                1.0          0.027409\n",
      "0     amakovec@fnal.gov    967    967                1.0          0.012980\n",
      "23    taniuchi@fnal.gov    534    534                1.0          0.007168\n",
      "15     mu2epro@fnal.gov    175    175                1.0          0.002349\n",
      "16     normanm@fnal.gov    119    119                1.0          0.001597\n",
      "4      dunepro@fnal.gov    118    118                1.0          0.001584\n",
      "19         osg@fnal.gov     84     84                1.0          0.001128\n",
      "7     granados@fnal.gov     64     64                1.0          0.000859\n",
      "3      dunegli@fnal.gov     33     33                1.0          0.000443\n",
      "2       dhayaa@fnal.gov     32     32                1.0          0.000430\n",
      "14    mmehmood@fnal.gov     22     22                1.0          0.000295\n",
      "25     vincent@fnal.gov     16     16                1.0          0.000215\n",
      "20     preimer@fnal.gov     15     15                1.0          0.000201\n",
      "26     vnagasl@fnal.gov     15     15                1.0          0.000201\n",
      "21     rhowell@fnal.gov     10     10                1.0          0.000134\n",
      "12      lperes@fnal.gov      7      7                1.0          0.000094\n",
      "8          hcc@fnal.gov      6      6                1.0          0.000081\n",
      "13  minervapro@fnal.gov      5      5                1.0          0.000067\n",
      "22      shivam@fnal.gov      5      5                1.0          0.000067\n",
      "18    omorenop@fnal.gov      1      1                1.0          0.000013\n",
      "27        zdar@fnal.gov      1      1                1.0          0.000013\n",
      "\n",
      "Ragged JSONL written to: ../data/anon_ragged.jsonl\n"
     ]
    }
   ],
   "source": [
    "# --- Imports\n",
    "import os, hmac, hashlib, base64, ipaddress, json\n",
    "from typing import Any, Dict, Iterable, List, Tuple, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "DATA_DIR = \"../data\"\n",
    "\n",
    "# ==========\n",
    "# 0) Key setup (replace with KMS in prod)\n",
    "# ==========\n",
    "KEY_HEX = hashlib.sha256(b\"random-demo-key\").hexdigest()\n",
    "KEY = bytes.fromhex(KEY_HEX)\n",
    "\n",
    "# ==========\n",
    "# 1) HMAC tokenization (deterministic pseudonym)\n",
    "# ==========\n",
    "def _to_bytes(x: Any) -> bytes:\n",
    "    if x is None:\n",
    "        return b\"\"\n",
    "    try:\n",
    "        if pd.isna(x):\n",
    "            return b\"\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    if isinstance(x, bytes):\n",
    "        return x\n",
    "    return str(x).encode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "def hmac_token(value: Any, key: bytes, out_len: int = 22) -> str:\n",
    "    mac = hmac.new(key, _to_bytes(value), hashlib.sha256).digest()\n",
    "    return base64.b32encode(mac).decode(\"ascii\").rstrip(\"=\")[:out_len].lower()\n",
    "\n",
    "# ==========\n",
    "# 2) IPv4 anonymization (deterministic 32-bit remap → dotted-quad)\n",
    "#    Note: This preserves IPv4 format, not subnet/prefix structure (MVP-friendly).\n",
    "# ==========\n",
    "def ip_str_to_int(ip_str: str) -> Optional[int]:\n",
    "    if ip_str is None or (isinstance(ip_str, float) and np.isnan(ip_str)):\n",
    "        return None\n",
    "    try:\n",
    "        return int(ipaddress.IPv4Address(str(ip_str)))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def int_to_ip_str(ip_int: int) -> str:\n",
    "    return str(ipaddress.IPv4Address(ip_int & 0xFFFFFFFF))\n",
    "\n",
    "def anon_ipv4(ip_value: Any, key: bytes) -> Optional[str]:\n",
    "    \"\"\"Map any IPv4 string (or int-like) to a deterministic anon IPv4 string.\"\"\"\n",
    "    if ip_value is None or (isinstance(ip_value, float) and np.isnan(ip_value)):\n",
    "        return None\n",
    "    # Normalize to 32-bit int\n",
    "    ip_int = None\n",
    "    if isinstance(ip_value, (int, np.integer)):\n",
    "        ip_int = int(ip_value)\n",
    "    else:\n",
    "        ip_int = ip_str_to_int(str(ip_value))\n",
    "    if ip_int is None:\n",
    "        return None\n",
    "    mac = hmac.new(key, ip_int.to_bytes(4, \"big\", signed=False), hashlib.sha256).digest()\n",
    "    anon_int = int.from_bytes(mac[:4], \"big\")  # 32-bit remap\n",
    "    return int_to_ip_str(anon_int)\n",
    "\n",
    "def anon_ipv4_both(ip_value: Any, key: bytes) -> Tuple[Optional[str], Optional[int]]:\n",
    "    \"\"\"Return (anon_ip_str, anon_ip_int) for convenience.\"\"\"\n",
    "    s = anon_ipv4(ip_value, key)\n",
    "    return s, (int(ipaddress.IPv4Address(s)) if s else None)\n",
    "\n",
    "# ==========\n",
    "# 3) Assignment functions\n",
    "# ==========\n",
    "\n",
    "# 1) Anon the user field and create a mapping\n",
    "def anonymize_user_with_mapping(\n",
    "    df: pd.DataFrame, user_col: str, key: bytes = KEY, token_len: int = 22\n",
    ") -> Tuple[pd.Series, Dict[Any, str]]:\n",
    "    mapping: Dict[Any, str] = {}\n",
    "    def _map(v):\n",
    "        if v in mapping:\n",
    "            return mapping[v]\n",
    "        t = hmac_token(v, key, out_len=token_len) if (v is not None and not (isinstance(v, float) and np.isnan(v))) else v\n",
    "        mapping[v] = t\n",
    "        return t\n",
    "    anon_series = df[user_col].map(_map)\n",
    "    return anon_series, mapping\n",
    "\n",
    "# 2) Anon the IP address field (works if your data is strings or 32-bit ints)\n",
    "def anonymize_ip_with_mapping(\n",
    "    df: pd.DataFrame, ip_col: str, key: bytes = KEY\n",
    ") -> Tuple[pd.Series, Dict[Any, Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      anon_ip_str_series,\n",
    "      mapping: { original -> {\"anon_ip\": str, \"anon_ip_int\": int} }\n",
    "    \"\"\"\n",
    "    mapping: Dict[Any, Dict[str, Any]] = {}\n",
    "    def _map(v):\n",
    "        if v in mapping:\n",
    "            return mapping[v][\"anon_ip\"]\n",
    "        s, i = anon_ipv4_both(v, key)\n",
    "        mapping[v] = {\"anon_ip\": s, \"anon_ip_int\": i}\n",
    "        return s\n",
    "    anon_series = df[ip_col].map(_map)\n",
    "    return anon_series, mapping\n",
    "\n",
    "# 3) Make vectors of the anonymous data and clear data\n",
    "def make_vectors(\n",
    "    df: pd.DataFrame,\n",
    "    anon_cols: Iterable[str],\n",
    "    clear_cols: Iterable[str],\n",
    "    user_col: Optional[str] = None,\n",
    "    ip_col: Optional[str] = None,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Build simple numeric vectors:\n",
    "      - Strings are hashed to integers (stable) with SHA256 and truncated.\n",
    "      - Numbers remain as-is; datetime → epoch ms; bool → 0/1; None/NaN → -1.\n",
    "    Produces (X_anon, X_clear) aligned by row.\n",
    "    \"\"\"\n",
    "    def _numify(val: Any) -> float:\n",
    "        if val is None or (isinstance(val, float) and np.isnan(val)):\n",
    "            return -1.0\n",
    "        if isinstance(val, (int, float, np.integer, np.floating)):\n",
    "            return float(val)\n",
    "        if isinstance(val, (pd.Timestamp, np.datetime64)):\n",
    "            try:\n",
    "                return pd.to_datetime(val).value / 1e6  # ms\n",
    "            except Exception:\n",
    "                pass\n",
    "        if isinstance(val, (bool, np.bool_)):\n",
    "            return 1.0 if val else 0.0\n",
    "        # strings/other → deterministic hash to 32-bit range\n",
    "        h = hashlib.sha256(_to_bytes(val)).digest()\n",
    "        return float(int.from_bytes(h[:4], \"big\"))  # 0..2^32-1\n",
    "\n",
    "    X_anon = np.array([[ _numify(df.loc[i, c]) for c in anon_cols ] for i in df.index], dtype=float)\n",
    "    X_clear = np.array([[ _numify(df.loc[i, c]) for c in clear_cols ] for i in df.index], dtype=float)\n",
    "    return X_anon, X_clear\n",
    "\n",
    "# 4) Generate a ragged array (variable-length dump)\n",
    "def to_ragged_records(df: pd.DataFrame, include_cols: Iterable[str]) -> List[List[Tuple[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Represent each record as a list of (key, value) for only present (non-null) columns.\n",
    "    This yields variable-length records → a ragged array.\n",
    "    \"\"\"\n",
    "    ragged: List[List[Tuple[str, Any]]] = []\n",
    "    for _, row in df.iterrows():\n",
    "        items = []\n",
    "        for c in include_cols:\n",
    "            v = row.get(c, None)\n",
    "            if v is not None and not (isinstance(v, float) and np.isnan(v)):\n",
    "                items.append((c, v))\n",
    "        ragged.append(items)\n",
    "    return ragged\n",
    "\n",
    "def dump_ragged_jsonl(ragged: List[List[Tuple[str, Any]]], path: str) -> None:\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for rec in ragged:\n",
    "            # dump as list of {\"k\":..., \"v\":...} for clarity\n",
    "            f.write(json.dumps([{\"k\": k, \"v\": v} for k, v in rec], ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# Bonus) Failure stats per user (absolute and fraction of dataset)\n",
    "def failure_stats(\n",
    "    df: pd.DataFrame,\n",
    "    user_col: str,\n",
    "    failed_col: str,          # boolean or {0/1} or {\"success\"/\"failed\"} etc.\n",
    "    normalize: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with:\n",
    "      user, fails, total, frac_of_user_rows, frac_of_all_rows\n",
    "    \"\"\"\n",
    "    # normalize to boolean\n",
    "    failed = df[failed_col]\n",
    "    if failed.dtype == \"O\":\n",
    "        failed_bool = failed.astype(str).str.lower().isin([\"1\",\"true\",\"t\",\"yes\",\"y\",\"failed\",\"fail\"])\n",
    "    else:\n",
    "        failed_bool = failed.astype(bool)\n",
    "\n",
    "    g = df.assign(__failed=failed_bool).groupby(user_col, dropna=False)\n",
    "    per_user = g[\"__failed\"].agg(fails=\"sum\", total=\"count\")\n",
    "    per_user[\"frac_of_user_rows\"] = per_user[\"fails\"] / per_user[\"total\"]\n",
    "    per_user = per_user.reset_index()\n",
    "    if normalize:\n",
    "        all_total = len(df)\n",
    "        per_user[\"frac_of_all_rows\"] = per_user[\"fails\"] / max(all_total, 1)\n",
    "    else:\n",
    "        per_user[\"frac_of_all_rows\"] = np.nan\n",
    "    return per_user.sort_values([\"fails\",\"total\"], ascending=[False, False])\n",
    "\n",
    "# ==========\n",
    "# Demo wiring (remove/adjust for your data)\n",
    "# ==========\n",
    "# Example schema guess; rename these to match your columns:\n",
    "anonymized_columns = [\"x509UserProxyEmail\",\"User\",\"JobsubClientIpAddress\"]\n",
    "USER_COL = \"User\"\n",
    "IP_COL   = \"JobsubClientIpAddress\"        # can be IPv4 string or 32-bit int in your data\n",
    "FAILED_COL = \"DAG_NodesFailed\"  # boolean-ish; see failure_stats()\n",
    "\n",
    "# Minimal demo DataFrame (replace with your read_parquet)\n",
    "fnames_in = f\"{DATA_DIR}/fifebatch-history-*.parquet\"\n",
    "files = glob.glob(fnames_in)\n",
    "print(f\"Found {len(files)} files\")\n",
    "n_files = 2\n",
    "# Read and concatenate\n",
    "demo = pd.concat([pd.read_parquet(f, engine=\"fastparquet\") for f in files[:n_files]], ignore_index=True)\n",
    "print(f\"Read {len(files[:n_files])} and loaded demo:{len(demo)}\")\n",
    "# demo = pd.DataFrame({\n",
    "#     USER_COL:   [\"alice\",\"bob\",\"alice\",\"carol\", None],\n",
    "#     IP_COL:     [\"10.1.2.3\", \"10.1.2.4\", 3232235777, \"192.168.1.10\", \"not-an-ip\"],  # third is 192.168.1.1 as int\n",
    "#     FAILED_COL: [True, False, True, False, True],\n",
    "#     \"job_id\":   [1,2,3,4,5],\n",
    "# })\n",
    "\n",
    "# 1) User anon + mapping\n",
    "demo[\"user_anon\"], user_map = anonymize_user_with_mapping(demo, USER_COL, key=KEY)\n",
    "print(\"1) Anonymized user with mapping\")\n",
    "# 2) IP anon + mapping\n",
    "demo[\"ip_anon\"], ip_map = anonymize_ip_with_mapping(demo, IP_COL, key=KEY)\n",
    "print(\"2) Anonymized IP with mapping\")\n",
    "# 3) Vectors (choose any columns you want included)\n",
    "anon_cols  = [\"user_anon\",\"ip_anon\"]\n",
    "clear_cols = [USER_COL, IP_COL, FAILED_COL]\n",
    "X_anon, X_clear = make_vectors(demo, anon_cols=anon_cols, clear_cols=clear_cols)\n",
    "print(\"2) Made vectors\")\n",
    "# 4) Ragged array dump (variable-length records)\n",
    "ragged = to_ragged_records(demo, include_cols=[USER_COL,\"user_anon\",IP_COL,\"ip_anon\",\"job_id\",FAILED_COL])\n",
    "dump_ragged_jsonl(ragged, f\"{DATA_DIR}/anon_ragged.jsonl\")\n",
    "print(\"4) Made ragged array\")\n",
    "# Bonus) Failure stats per user\n",
    "fails = failure_stats(demo, user_col=USER_COL, failed_col=FAILED_COL, normalize=True)\n",
    "\n",
    "print(\"=== Anonymized preview ===\")\n",
    "print(demo)\n",
    "print(\"\\n=== User mapping (first 3) ===\")\n",
    "print(dict(list(user_map.items())[:3]))\n",
    "print(\"\\n=== IP mapping (first 3) ===\")\n",
    "print(dict(list(ip_map.items())[:3]))\n",
    "print(\"\\n=== X_anon shape / X_clear shape ===\", X_anon.shape, X_clear.shape)\n",
    "print(\"\\n=== Failure stats ===\")\n",
    "print(fails)\n",
    "\n",
    "print(f\"\\nRagged JSONL written to: {DATA_DIR}/anon_ragged.jsonl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
